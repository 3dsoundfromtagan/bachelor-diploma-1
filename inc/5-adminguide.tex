\section{Руководство администратора}

\subsection{Расположение серверов в сетевой инфраструктуре}
Во всей инфраструктуре на физических серверах первого дата-центра располагаются:
\begin{itemize}
  \item shared-хостинг (10.0.0.100);
  \item виртуализация OpenVZ (10.0.1.100);
  \item виртуализация KVM (10.0.2.100);
  \item сервер резервного копирования (10.0.3.100);
  \item физические сервера клиентов (10.0.4.100 --- 10.0.4.200).
\end{itemize}

В свою очередь некоторые важные сервисы инфраструктуры располагаются на виртуальных машинах, такие как:
\begin{itemize}
  \item сервер биллинга (10.0.1.101), OpenVZ;
  \item сервер управления IP-адесами (10.0.2.101), KVM;
  \item сервер DNS 1 (10.0.2.102), KVM.
\end{itemize}

Сервер мониторинга и сервер DNS 2 располагаются в виртуальных машинах KVM в другом дата-центре.
Сервер shared-хостинга выполняет роль главного (master) DNS-сервера.

\subsection{Сервер shared-хостинга}

Сервер shared-хостинга является самым уязвимым местом всей инфраструктуры из-за большого количества и плотности клиентов на сервере.
Распределение ресурсов происходит за счет встроенных в ядро Linux механизмов.

На сервере установлен и настроен веб-сервер в составе следующего ПО:
\begin{itemize}
  \item роль http-сервера выполняет связка между Apache HTTP Server, кэширующего и проксирующего http-сервера nginx и обработчика динамических запросов php-fpm;
  \item memcached кэширует динамические запросы в оперативной памяти;
  \item роли серверов баз данных выполняют MySQL и PostgreSQL.
\end{itemize}

nginx принимает запросы к серверу с порта 80 и решает каким образом обрабатывать запрос, если это запрос на получение статического файла, то nginx сам его обрабатывает и в случае необходимости кэширует.
Если же запрашивается динамическое содержимое, то nginx передает запрос на бэкенд, где его, в зависимости от настройки виртуального хоста сайта обрабатывает либо php-fpm, либо встроенный модуль Apache mod\_php.
nginx не может кэшировать динамические запросы, поэтому для этого имеется memcached, который работает в связке с Apache и позволяет кэшировать динамические запросы.
Для правильной работы Apache с memcached необходима установка модуля PHP php5-memcached:
\begin{lstlisting}
# apt-get install php5-memcached
\end{lstlisting}

В качестве связки для работы почты выступает SMTP-сервер Exim Internet Mailer, запросы по протоколам POP3 и IMAP принимает Dovecot.
В качестве антиспам-связки выступает Postgrey, Spamassasin, ClamAV и OpenDKIM.

ProFTPd необходим для работы пользователей по протоколу FTP.

Сервер управления временем NTP обращается к следующим серверам за уточнением времени на сервере:
\begin{lstlisting}
# cat /etc/ntp.conf | grep ^server | awk '{print $2}'
0.debian.pool.ntp.org
1.debian.pool.ntp.org
2.debian.pool.ntp.org
3.debian.pool.ntp.org
\end{lstlisting}

Конфигурация SSH требует отдельного пояснения.
Доступ к SSH по стандартному порту закрыт, это позволяет избавиться от большинства злоумышленников, которые пытаются скомпрометировать пароль доступа к серверу.
Также запрещен вход от пользователя root, для этого создан отдельный пользователь.
Также можно ограничить доступ к серверу по SSH, оставив только доверенные адреса или подсети адресов.
\begin{lstlisting}
Adding user for SSH:
# useradd sshuser
# passwd sshuser
SSH config:
# cat /etc/ssh/sshd_config
#change SSH port 222:
Port 222
#close root login:
PermitRootLogin yes
#allow IP-addresses:
Match host 10.0.0.111, 10.0.0.222
  #root for allowed IP's:
  PermitRootLogin yes
Restart SSH:
# service ssh restart
\end{lstlisting}

Разрешен вход на сервер по ключам:
\begin{lstlisting}
Key Generating:
user@10.0.0.111~$ ssh-keygen
user@10.0.0.111~$ ssh-keygen -p
Copy private key to server:
user@10.0.0.111~$ ssh-copy-id -p 222 root@10.0.0.100
Reconnect to server:
user@10.0.0.111~$ ssh -p 222 root@10.0.0.100
\end{lstlisting}

Резервные копии с сервера делаются средствами панели ISPmanager в ночное время, применяется инкрементальный метод резервного копирования, когда сначала делается полная копия, а затем инкрементальные копии только измененных данных.

На сервере функционирует скрипт блокировки адресов, которые слишком часто подбирают пароли доступа к панели администратора наиболее популярных CMS, таких как WordPress и Joomla.
Скрипт находится в открытом доступе и имеет простое использование:
\begin{lstlisting}
# cat /etc/nginx/blockips.sh
#!/bin/bash
# admin@amet13.name
# v0.2 30.06.2014
command=$(cat /var/log/nginx/access.log | \
grep "administrator/index.php \|wp-login.php " | \
cut -f1 -d " " | sort | \
uniq -c | sort -n | \
awk '{ if ($1 > 2999) print $2}')
# Add new ip's to database
for word in $command; do
   # Is that ip already in database?
   grep $word /etc/nginx/blockips.conf > /dev/null
   if (( $? )); then 
      sed -i "s/allow all;/deny $word;\nallow all;/" \
      /etc/nginx/blockips.conf
   fi
done
exit 0
\end{lstlisting}

Основные конфигурации настроек веб-сервера хранятся в:
\begin{lstlisting}
/etc/nginx/nginx.conf
/etc/apache2/apache2.conf
/etc/php5/{apache2,cgi,cli,fpm}/php.ini
/etc/mysql/my.cnf
/etc/postgresql/9.1/main/postgresql.conf
\end{lstlisting}

Для сканирования уязвимостей на сайтах клиентов используется утилита maldet, которая находит подозрительные файлы в системе и составляет отчет по найденным файлам.
Принцип пользования утилитой:
\begin{lstlisting}
Instalaltion:
# cd /tmp/
# wget http://www.rfxn.com/downloads/maldetect-current.tar.gz
# tar xfz maldetect-current.tar.gz
# cd maldetect-*
# ./install.sh
Run scanning (-b -- background mode):
# maldet -b --scan-all /var/www/
Show reports list:
# maldet --report list
Report view:
# maldet --report 021715-1414.3266
Send infected files to quarantine:
# maldet -q 021715-1414.3266
\end{lstlisting}

На сервере shared-хостинга в сайтах пользователей большое количество уязвимостей, которые часто используются для рассылки спама с сервера.
Для обнаружения большого числа рассылок с сервера написан плагин для системы мониторинга Nagios, который контролирует число писем в почтовой очереди, а также проверяет наиболее популярные организации, которые собирают данные о спам-серверах.

Для просмотра списка очереди используется команда exim -bp или mailq.
Таким образом можно просмотреть количество писем для разных доменов и их количество:
\begin{lstlisting}
# exim -bp | exiqsumm -c -s | head -10
Count  Volume  Oldest  Newest  Domain
-----  ------  ------  ------  ------
12151  1378KB     14h      0m  spamsite.ru > host.ru
   93   264KB     14h      5m  spamsite.ru > gmail.com
   16   154KB     13h     11h  site.ru > kk.com
    8    77KB     13h     11h  site.ru > ww.com
...
\end{lstlisting}

В данном случае очевидна рассылка писем с сайта spamsite.ru.
Посмотреть ID письма а также содержимое заголовков и тела письма можно командами:
\begin{lstlisting}
# exiqgrep -b -f 'spamsite.ru' | head -1
1YyRgs-0000zD-Qy From: <fobos@spamsite.ru> To: maria@host.ru
# exim -Mvh 1YyRgs-0000zD-Qy
# exim -Mvb 1YyRgs-0000zD-Qy
\end{lstlisting}

Если сайт действительно заражен, то следует отключить его, оповестить пользователя о закрытии сайта и необходимости избавиться от вредоносного кода и удалить очередь сообщений из этих писем:
\begin{lstlisting}
# exiqgrep -b -f 'spamsite.ru' | awk '{print $1}' | xargs exim -Mrm
\end{lstlisting}

В случае неработоспособности панели ISPmanager 5 возможна ее перезагрузка:
\begin{lstlisting}
# pgrep -l core
4437 core
7129 core
21194 core
# killall core
\end{lstlisting}

Обновление компонентов системы:
\begin{lstlisting}
# apt-get update
# apt-get upgrade
\end{lstlisting}

\subsection{Сервер виртуализации OpenVZ}

Список существующих в системе контейнеров можно узнать командой vzlist:
\begin{lstlisting}
# vzlist -a
      CTID      NPROC STATUS    IP_ADDR      HOSTNAME
       145        134 running   10.0.0.245   site1.ru
       146         41 running   10.0.0.246   site2.ru
       205          - stopped   10.0.0.205   site3.ru
...
\end{lstlisting}

В заголовках показывается показывается ID контейнера, число процессов в контейнере, текущий статус, IP-адрес и имя контейнера.

Список всех доступных шаблонов операционной системы и список шаблонов установленных локально:
\begin{lstlisting}
# vztmpl-dl --list-all
...
# vztmpl-dl --list-local
centos-7-x86_64
debian-7.0-x86_64
ubuntu-14.04-x86_64
\end{lstlisting}

Каждый контейнер имеет свой конфигурационный файл, который хранится в каталоге /etc/sysconfig/vz-scripts/.
Именуются эти файлы по CTID контейнера.
Например, для контейнера с CTID=101, файл будет называться 101.conf.

При создании контейнера можно использовать типовую конфигурацию для VPS.
Типовые файлы конфигураций находятся в том же каталоге:
\begin{lstlisting}
# ls -1 /etc/sysconfig/vz-scripts/ | grep sample
ve-basic.conf-sample
ve-ispbasic.conf-sample
ve-light.conf-sample
ve-vswap-1024m.conf-sample
ve-vswap-1g.conf-sample
ve-vswap-256m.conf-sample
ve-vswap-2g.conf-sample
ve-vswap-4g.conf-sample
ve-vswap-512m.conf-sample
\end{lstlisting}

В этих конфигурационных файлах описаны контрольные параметры ресурсов, выделенное дисковое пространство, оперативная память и прочие ресурсы.
На базе уже существующего фала конфигурации создан типовой файл, подходящий для большинства контейнеров именуемый ve-custom.conf-sample.
Таким образом, при использовании этого конфигурационного файла, будет создаваться контейнер, которому будет доступен 1 Гб выделенного дискового пространства, 128 Мб оперативной памяти и 128 Мб swap.
В дальнейшем, при создании контейнеров следует использовать данный конфигурационный файл.

Создание контейнера:
\begin{lstlisting}
# vzctl create 101 --ostemplate debian-7.0-x86_64 --config custom
\end{lstlisting}
где 101 --- CTID контейнера, -{}-ostemplate --- шаблон ОС, -{}-config --- используемый шаблон конфигурационного файла.

Первым запуском контейнера необходимо установить его IP адрес, hostname, указать DNS сервер и задать пароль суперпользователя.

Для настройки VPS используется команда vzctl set.
Для того, чтобы контейнер запускался при старте хост-компьютера (например после перезагрузки), необходимо использовать команду:
\begin{lstlisting}
# vzctl set 101 --onboot yes --save
CT configuration saved to /etc/vz/conf/101.conf
\end{lstlisting}

При использовании ключа -{}-save, сохраняются параметры контейнера в соответствующий конфигурационный файл.
Аналогично можно задать hostname:
\begin{lstlisting}
# vzctl set 101 --hostname site.com --save
CT configuration saved to /etc/vz/conf/101.conf
\end{lstlisting}

Установка IP адреса:
\begin{lstlisting}
# vzctl set 101 --ipadd 10.0.0.210 --save
CT configuration saved to /etc/vz/conf/101.conf
\end{lstlisting}

Адрес DNS сервера (в большинстве случаев адрес DNS совпадает с адресом хост-компьютера, поэтому можно вместо адреса указать параметр inherit):
\begin{lstlisting}
# vzctl set 101 --nameserver inherit --save
CT configuration saved to /etc/vz/conf/101.conf
\end{lstlisting}

Установка пароля суперпользователя:
\begin{lstlisting}
# vzctl set 101 --userpasswd root:p@ssw0rd
Starting container...
...
Unmounting file system at /vz/root/101
Unmounting device /dev/ploop37965
Container is unmounted
\end{lstlisting}

Пароль будет установлен в VPS, в файл \texttt{/etc/shadow} и не будет сохранен в конфигурационный файл контейнера.
Если же пароль будет утерян или забыт, то можно будет просто задать новый.

После настроек нового контейнера, его можно запустить:
\begin{lstlisting}
# vzctl start 101
Starting container...
Opening delta /vz/private/101/root.hdd/root.hdd
Adding delta dev=/dev/ploop37965 img=/vz/private/101/root.hdd/root.hdd (rw)
Mounting /dev/ploop37965p1 at /vz/root/101 fstype=ext4 data='balloon_ino=12,'
Container is mounted
Adding IP address(es): 10.0.0.210
Setting CPU units: 1000
Container start in progress...
\end{lstlisting}

Проверка сетевых интерфейсов внутри гостевой ОС:
\begin{lstlisting}
# vzctl exec 101 ifconfig | grep "lo\|venet" -A 1
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
--
venet0    Link encap:UNSPEC  HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00
          inet addr:127.0.0.2  P-t-P:127.0.0.2  Bcast:0.0.0.0  Mask:255.255.255.255
--
venet0:0  Link encap:UNSPEC  HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00
          inet addr:10.0.0.210  P-t-P:192.168.0.101  Bcast:192.168.0.101  Mask:255.255.255.255
\end{lstlisting}

Должны присутствовать сетевые интерфейсы:
\begin{itemize}
    \item lo (127.0.0.1);
    \item venet0 (127.0.0.2);
    \item venet0:0 (10.0.0.210).
\end{itemize}

Если сеть в порядке, то можно соединиться к контейнеру по SSH с хост-компьютера:
\begin{lstlisting}
# ssh root@10.0.0.210
root@10.0.0.210's password: p@ssw0rd
\end{lstlisting}

Вход в контейнер напрямую с хост-компьютера осуществляется командой vzctl enter:
\begin{lstlisting}
# vzctl enter 101
entered into CT 101
root@site:/#
\end{lstlisting}

Выход из контейнера:
\begin{lstlisting}
root@site:/#  exit
logout
exited from CT 101
\end{lstlisting}

Для остановки контейнера используется команда vzctl stop.
Для полной остановки контейнера, системе требуется немного времени.
Иногда нужно выключить VPS как можно быстрее, например, если контейнер был подвержен взлому.
Для того чтобы срочно выключить VPS, нужно использовать ключ -{}-fast:
\begin{lstlisting}
# vzctl stop 101 --fast
Killing container ...
Container was stopped
Unmounting file system at /vz/root/101
Unmounting device /dev/ploop37965
Container is unmounted
\end{lstlisting}

Для перезапуска контейнера можно использовать команду vzctl restart.

Для того чтобы удалить контейнер, его нужно сначала остановить:
\begin{lstlisting}
# vzctl stop 101
Stopping container ...
Container was stopped
Unmounting file system at /vz/root/101
Unmounting device /dev/ploop37965
Container is unmounted
\end{lstlisting}

Для удаления используется команда:
\begin{lstlisting}
# vzctl destroy 101
CTID 101 deleted unmounted down
\end{lstlisting}

Команда выполняет удаление частной области сервера и переименовывает файл конфигурации, дописывая к нему .destroyed.

Иногда бывает нужно выполнить команду на нескольких VPS.
Для этого можно использовать команду:
\begin{lstlisting}
# for i in `vzlist -o veid -H`; do \
> echo "VPS $i"; vzctl exec $i command; done
\end{lstlisting}

Например, можно узнать, сколько времени работают все запущенные контейнеры:
\begin{lstlisting}
# for i in `vzlist -o veid -H`; do \
> echo "VPS $i"; vzctl exec $i uptime; done
VPS 101
 05:45:01 up 2 min, 0 users, load average: 0.01, 0.02, 0.03
VPS 102
 05:46:01 up 1 min, 0 users, load average: 0.04, 0.05, 0.06
\end{lstlisting}

Если на хост-ноде наблюдается высокое значение Load Average, то в первую очередь следует посмотреть данные значения непосредственно у контейнеров:
\begin{lstlisting}
# vzlist -o veid,laverage
      CTID       LAVERAGE
       145 0.00/0.01/0.05
       146 0.00/0.00/0.00
       148 0.60/0.51/0.34
...
\end{lstlisting}

В случае обнаружения нагружающего контейнера, стоит войти в него и устранить причину нагрузки.

Непосредственно с хост-ноды можно узнать, какому контейнеру принадлежит процесс:
\begin{lstlisting}
# vzpid 1048108
Pid	    CTID	Name
1048108	145	  php-cgi
\end{lstlisting}

Свободное место в контейнере можно узнать командой df:
\begin{lstlisting}
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg_id3611-lv_root
                       50G  5,6G   42G  12% /
tmpfs                  16G     0   16G   0% /dev/shm
/dev/cciss/c0d0p1     485M  276M  184M  61% /boot
/dev/mapper/vg_id3611-lv_vz
                      489G  403G   62G  87% /vz

# df -i
Filesystem             Inodes IUsed    IFree IUse% Mounted on
/dev/mapper/vg_id3611-lv_root
                      3276800 55229  3221571    2% /
tmpfs                 4101290     1  4101289    1% /dev/shm
/dev/cciss/c0d0p1      128016    96   127920    1% /boot
/dev/mapper/vg_id3611-lv_vz
                     32546816   248 32546568    1% /vz
\end{lstlisting}

Подключение модуля TUN для контейнера (необходимо для работы OpenVPN.
Прежде чем запускать контейнер нужно убедиться, что модуль TUN загружен на хост-ноде:
\begin{lstlisting}
# lsmod | grep tun
\end{lstlisting}

В случае, если он не загружен, загрузить его можно следующей командой:
\begin{lstlisting}
# modprobe tun
# lsmod | grep tun
tun     1957    0
\end{lstlisting}

Разрешаем использовать устройство TUN контейнеру:
\begin{lstlisting}
# vzctl set 101 --devnodes net/tun:rw --save
CT configuration saved to /etc/vz/conf/101.conf
# vzctl set 101 --devices c:10:200:rw --save
CT configuration saved to /etc/vz/conf/101.conf
# vzctl set 101 --capability net_admin:on --save
CT configuration saved to /etc/vz/conf/101.conf
\end{lstlisting}

Запускаем контейнер:
\begin{lstlisting}
# vzctl start 101
Starting container...
...
Container start in progress...
\end{lstlisting}

Создаем в контейнере собственное устройство TUN:
\begin{lstlisting}
# vzctl exec 101 mkdir -p /dev/net
# vzctl exec 101 mknod /dev/net/tun c 10 200
# vzctl exec 101 chmod 600 /dev/net/tun
\end{lstlisting}

Для пользователей в контейнерах доступен файерволл IPTables, для его работы в файле /etc/vz/vz.conf должна присутствовать строка:
\begin{lstlisting}
IPTABLES="ipt_owner ipt_REDIRECT ipt_recent ip_tables iptable_filter iptable_mangle ipt_limit ipt_multiport ipt_tos ipt_TOS ipt_REJECT ipt_TCPMSS ipt_tcpmss ipt_ttl ipt_LOG ipt_length ip_conntrack ip_conntrack_ftp ipt_state iptable_nat ip_nat_ftp"
\end{lstlisting}

ля того, чтобы для контейнеров был доступен FUSE, его необходимо включить на хост-ноде:
\begin{lstlisting}
# modprobe fuse
\end{lstlisting}

Проверить, что модуль успешно подключен:
\begin{lstlisting}
# lsmod | grep fuse
fuse  92980  54
\end{lstlisting}

Также необходимо добавить автозагрузку модуля при перезапуске хост-ноды:
\begin{lstlisting}
# vim /etc/rc.local
#!/bin/sh
...
modprobe fuse
\end{lstlisting}

Проброс FUSE для контейнера 101:
\begin{lstlisting}
# vzctl stop 101
# vzctl set 101 --devnodes fuse:rw --save
# vzctl set 101 --devices c:10:229:rw --save
# vzctl start 101
\end{lstlisting}

В контейнере проверяем, пробросилось ли устройство:
\begin{lstlisting}
[root@site /]# ls /dev/fuse
/dev/fuse
\end{lstlisting}

Резервное копирование контейнеров осуществляется утилитами vzdump и vzrestore:
\begin{lstlisting}
# vzdump --suspend --compress --dumpdir /root/ --exclude-path /tmp/ 101
# vzrestore /vz/dump/vzdump-openvz-101-2015_01_01-12_12_12.tar 103
\end{lstlisting}

Для работы OpenVZ с файлами используется ploop, так как simfs является устаревшим методом.
Диски с файловыми системами контейнеров хранятся в /vz/private и имеют имя root.hdd.

Смена размера диска для контейнера:
\begin{lstlisting}
# vzctl set 103 --diskspace 2G --save
dumpe2fs 1.41.12 (17-May-2010)
Changing balloon size old_size=1182793728 new_size
=217055232
Successfully truncated balloon from 1182793728 to 217055232
bytes
UB limits were set successfully
CT configuration saved to /etc/vz/conf/103.conf
\end{lstlisting}

\subsection{Сервер виртуализации KVM}

В целом, администрирование KVM-ноды не отличается многим от OpenVZ.
Однако в случае OpenVZ, возможно получить доступ к пользовательскому контейнеру непосредственно с хост-ноды, в случае с KVM это невозможно, это следует учесть при работе с пользователем.
Изменение размера диска также является нетривиальной задачей, высок риск краха всей файловой системы без возможности восстановления данных.

Команды, которые чаще всего используются при администрировании виртуальных машин на базе KVM:
\begin{lstlisting}
VM listing:
# virsh list --all
VM start:
# virsh start win_templ
VM shutdown:
# virsh shutdown win_templ
# virsh destroy win_templ
VM destroy:
# virsh undefine win_templ
VM enable autostart:
# virsh autostart win_templ
VM disable autostart:
# virsh autostart --disable win_templ
Update VM info:
# virsh define /etc/libvirt/qemu/win_templ.xml
VNC port:
#virsh vncdisplay win_templ
# virsh dumpxml win_templ | grep vnc
\end{lstlisting}

Для защиты сервера от брутфорса пароля SSH на серверах OpenVZ и KVM используется fail2ban.
Установка (в CentOS 6, для установки fail2ban нужно подключить репозиторий EPEL):
\begin{lstlisting}
# rpm -ivh http://download.fedoraproject.org/pub/epel/6/$(arch)/epel-release-6-8.noarch.rpm
# yum install fail2ban
\end{lstlisting}

fail2ban блокирует на некоторое время с помощью IPTables активные адреса, которые пытаются соединиться по SSH после пяти неудачных попыток, также отправляет администратору письмо с уведомлением о блокировке:
\begin{lstlisting}
# cat /etc/fail2ban/jail.local
...
[ssh-iptables]
enabled = true
filter = sshd
action = iptables[name=SSH, port=ssh, protocol=tcp]
sendmail-whois[name=SSH, dest=admin@domain.com, sender=fail2ban@example.com, sendername="Fail2Ban"]
maxretry = 5
bantime = 86400
\end{lstlisting}

Разблокировать IP адрес из бана:
\begin{lstlisting}
# fail2ban-client set JAIL unbanip IP
\end{lstlisting}

Пример:
\begin{lstlisting}
# fail2ban-client set ssh-iptables unbanip 10.0.0.111
\end{lstlisting}

\subsection{Второстепенные сервера}

В качестве сервера мониторинга используется Nagios, для построения графиков --- Munin.
Дополнительной настройки этих сервисов не требуется.
Добавление новых хостов происходит вручную, требуется лишь править адреса и сервисы, которые требуется мониторить.

Важно мониторить свободное место на жестком диске сервера резервного копирования и вовремя очищать старые резервные копии.

В DNS серверах используется master-slave репликация, причем один из DNS-серверов, равно как и сервер мониторинга расположены в другом дата-центре для обеспечения отказоустойчивости системы.
Также между slave-серверами настроена балансировка нагрузки.

\subsection{Общие рекомендации по администрированию инфраструктуры}

Следует следить за новостными рассылками о критических уязвимостях в операционной системе и используемом программном обеспечении.

Если работа сервера замедлилась, в диагностике проблемы помогут утилиты ps, top, atop, htop.
В случае обнаружения сетевых проблем на помощью придут утилиты ping, traceroute, nmap, mtr, tcpdump, iftop.

Важно следить за состоянием RAID на сервере, в случае сбоя одного из дисков, следует обратиться к поддержке дата-центра с просьбой о замене диска.

Не следует просто так перезагружать сервер, рекомендуется это делать только в случаях если совсем ничего не помогает.

Стоит незамедлительно реагировать на DDoS-атаки, при обнаружении подозрительного трафика следует убедиться, что это действительно DDoS, а также поинтересоваться у дата-центра реакцию сетевого оборудования на нее.

Всегда проверять наличие актуальных резервных копий, свободного места на жестких дисках.
Стоит также настроить ротацию логов, чтобы не занималось лишнее место на сервере.

На физических нодах стоит устанавливать только самый необходимый минимальный набор программного обеспечения, для уменьшения вероятности компрометации сервера.

Всегда стоит вести документацию в том или ином виде и логировать свои действия на сервере.

В случае обнаружения неполадок с аппаратной частью, стоит незамедлительно обратиться к своему провайдеру для проверки работоспособности аппаратуры.
В случае недочетов в программном обеспечении стоит обратиться к технической поддержке ПО.

Быстрое удаление правила iptables по его номеру.
Вывод правил цепочки INPUT с указанием номеров:
\begin{lstlisting}
# iptables -L INPUT --line-numbers
Chain INPUT (policy ACCEPT)
num  target     prot opt source               destination
1    DROP       tcp  --  anywhere             anywhere             tcp dpt:http STRING match  "/forbot" ALGO name kmp TO 65535
2    DROP       tcp  --  anywhere             anywhere             tcp dpt:http STRING match  "/tobot" ALGO name kmp TO 65535
3    DROP       tcp  --  anywhere             anywhere             tcp dpt:http STRING match  "save" ALGO name kmp TO 65535
\end{lstlisting}

Удаление правила с указанным номером:
\begin{lstlisting}
# iptables -D INPUT num
\end{lstlisting}


Восстановление пароля MySQL.
Останавливаем службу MySQL:
\begin{lstlisting}
# service mysql stop
\end{lstlisting}

Запускаем службу с опцией -{}-skip-grant-tables:
\begin{lstlisting}
# mysqld_safe --skip-grant-tables &
[1] 6518
Starting mysqld daemon with databases from /var/lib/mysql
mysqld_safe[5722]: started
\end{lstlisting}

Подключаемся с серверу MySQL при помощи клиента mysql:
\begin{lstlisting}
# mysql -u root
mysql>
\end{lstlisting}

Вводим новый пароль для root:
\begin{lstlisting}
mysql> use mysql;
mysql> update user set password=PASSWORD("NEW-PASS") where User='root';
mysql> flush privileges;
mysql> quit
\end{lstlisting}

Останавливаем сервер MySQL:
\begin{lstlisting}
# service mysql stop
\end{lstlisting}

Запускаем MySQL-сервер и логинимся с новым паролем:
\begin{lstlisting}
# service mysql start
# mysql -u root -p
Enter password: NEW-PASS
\end{lstlisting}


\clearpage
