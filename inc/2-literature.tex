\section{Обзор литературных источников}

\iffalse
Что я рассказал во введении:
\begin{itemize}
  \item что такое облачные услуги
  \item что такое iaas
  \item для чего нужна виртуализация
\end{itemize}

Примерно на 15-20 страниц.
\begin{itemize}
  \item облака + 
  \item ДЦ, типы для чего нужны tier iii + 
  \item виртуализация +
  \item виды виртуализации +
  \item контейнерная виртуализация +
  \item представители openvz, virtuozzo, docker, lxc, Linux-VServer, solaris zones, jails +
  \item использование
  \item перспективы в виртуализации
  \item ВСЕ ЛИ ЭТО???
\end{itemize}
\fi

Термин <<облачные вычисления>> сегодня уже достаточно хорошо известен и в информационных технологиях (IT), и в бизнес-кругах.
Почти каждую неделю появляются новые статьи, книги, презентации об облачных вычислениях --- новой сервисной модели предоставления вычислительных услуг.

За время существования информационных технологий сменилось несколько моделей построения информационных систем.
Все начиналось с монолитной архитектуры (mainframe), когда и база данных, и приложения работали на одном большом компьютере, а пользователи сидели у <<тонких>> терминалов, которые только отображали информацию.
У такой архитектуры было много недостатков, и ее сменили более перспективная архитектура <<клиент-сервер>>.
В ней был свой выделенный сервер баз данных, и пользователи на <<толстых>> клиентах, разгружая сервер БД.
Затем появилась еще более современная архитектура --- многоуровневая (или трехуровневая), где логика приложений была вынесена на отдельных компьютер, называемый сервером приложений, а пользователи работали на <<тонких>> клиентах через веб-браузеры.
Большинство приложений сегодня выполнено именно в этой архитектуре.
Она подразумевает развертывание всей IT-инфраструктуры на территории заказчика. \cite{oracle-db}

Облачные вычисления --- это следующий шаг в эволюции архитектуры построения информационных систем.
Благодаря огромным преимуществам этого подхода очевидно, что многие информационные системы в ближайшее время будут перенесены в облако.
Этот процесс уже начался, и его игнорирование или недооценка может привести к поражению в конкурентной борьбе на рынке.
Имеется ввиду не только отставание IT, или неоправданные затраты на него, но и отставание в развитии основного бизнеса компании, зависящего от гибкости IT-инфраструктуры и скорости вывода новых сервисов и продуктов на рынок.

IT-директор американского правительства Вивек Кундра, в феврале 2011 года опубликовал стратегию стратегию переноса части информационных систем в облако.
Документ под названием <<Federal Cloud Computing Strategy>> четко описывает порядок и сроки переноса. 
Цель работ --- уменьшение сложности и повышение управляемости IT, увеличение нагрузки оборудования до 70-80\%, уменьшение количества центров обработки данных.

Основным требованием, предъявляемым к центрам обработки данных является отказоустойчивость.
При этом подразумевается отключение ЦОД как на время планово-предупредительных работ и профилактики оборудования, так и внеплановых аварийных ситуаций.

Классификация Tier описывает надежность функционирования ЦОД и является необходимой для компаний, как желающих построить свой ЦОД, так и для арендующих чужие вычислительные мощности.
В зависимости от критичности бизнеса, в зависимости от потерь, которые понесет компания в случае остановки бизнес-процессов выбирается тот или иной уровень надежности.
В свою очередь, высокий уровень надежности требует высоких материальных и эксплуатационных затрат, поэтому и стоимость вычислительных мощностей зависит от уровня надежности ЦОД. \cite{dc-tier}

На сегодняшний день существует четыре уровня надежности ЦОД названные Tier I, Tier II, Tier III и Tier IV, которые были введены организацией Uptime Institute (Институт бесперебойных процессов, США):
\begin{itemize}
  \item Tier I: время простоя 28,8 часов в год, коэффициент отказоустойчивости 99,671\%;
  \item Tier II: 22,0 часа в год, 99,749\%;
  \item Tier III: 1,6 часа в год, 99,982\%;
  \item Tier IV: 0,4 часа в год, 99,995\%.
\end{itemize}

ЦОД уровня Tier I (базовый уровень) подвержен нарушениями работы как от плановых, так и от внеплановых действий.
Применение фальшпола, источников бесперебойного питания (ИБП), дизель-генераторных установок (ДГУ) не обязательно.
Если ИБП и ДГУ используются, то выбираются более простые модели, без резерва, с множеством точек отказа.
Возможны самопроизвольные отказы оборудования.
К простою ЦОД также приведут ошибки в действиях обслуживающего персонала.
В таких ЦОД отсутствует защита от случайных и намеренных событий, обусловленных действиями человека.

В ЦОД уровня Tier II (с резервированными компонентами) время простоя возможно в связи с плановыми и внеплановыми работами, а также аварийными ситуациями, однако оно сокращено благодаря внедрению одной резервной единицы оборудования в каждой системе.
Таким образом, системы кондиционирования, ИБП и ДГУ имеют одну резервную единицу, тем не менее, профилактические работы требуют отключения ЦОД.
В центрах обработки данных с резервированными компонентами требуется наличие минимальных защитных мер от влияния человека.

Третий уровень надежности (уровень с возможность параллельного проведения ремонтных работ) требует осуществления любой плановой деятельности без остановки ЦОД.
Под плановыми работами подразумевается профилактическое и программируемое техническое обслуживание, ремонт и замена компонентов, добавления или удаление компонентов, а также их тестирование.
В таком случае необходимо иметь резерв, благодаря которому можно пустить всю нагрузку по другому пути, во время работ на первом.
Для реализации Tier III необходима схема резервирования блоков схем кондиционирования, ИБП, ДГУ N+1, также требуется наличие двух комплектов трубопроводов для системы кондиционирования, построенной на основе чиллера (холодильной машины).
Строительные требования обязывают сохранять работоспособность ЦОД при большинстве случаев намеренных и случайных вмешательств человека.
Также следует предусмотреть резервные входы, дублирующие подъездные пути, контроль доступа, отсутствие окон, защиту от электромагнитного изучения.

Четвертый уровень надежности ЦОД (отказоустойчивый) характеризуется безостановочной работой при проведении плановых мероприятий и способен выдержать один серьезный отказ без последствий для критически важной нагрузки.
Необходим дублированный подвод питания, резервирование системы кондиционирования и ИБП по схеме 2(N+1).
Для дизель-генераторных установок необходима отдельная площадка с зоной хранения топлива.
Tier IV требует защиту от всех потенциальных проблем в связи с человеческим фактором.
Регламентированы избыточные средства защиты от намеренных или случайных действий человека.
Учтено влияние сейсмоявлений, потопов, пожаров, ураганов, штормов, терроризма.

Дата-центры по виду использования подразделяют на корпоративные и коммерческие (аутсорсинговые).
Корпоративные ДЦ предназначены для обслуживания конкретной компании, коммерческие, в свою очередь, предоставляет услуги всем желающим.

Некоторые ДЦ предлагают клиентам дополнительные услуги по использованию оборудования по автоматическому уходу от различных видов атак.
Команда специалистов круглосуточно производит мониторинг серверов.
Для обеспечения сохранности данных используются системы резервного копирования.
Для предотвращения кражи данных, в дата-центрах используются различные системы ограничения физического доступа, системы видеонаблюдения.

Дата-центры предоставляют несколько основных типов услуг, среди которых:
\begin{itemize}
  \item Виртуальный хостинг (shared hosting);
  \item Виртуальный сервер (virtual private/dedicated server);
  \item Выделенный сервер (dedicated server);
  \item Размещение сервера (colocation);
  \item Выделенная зона (dedicated area).
\end{itemize}

Виртуальный хостинг используется для размещения большого количества сайтов на одном веб-сервере.
В основном это типичный стек технологий LAMP, где в качестве операционной системы выступает GNU/Linux, веб-сервер Apache (зачастую в связке с nginx), сервер баз данных MySQL, PostgreSQL, интерпретируемые скриптовые языки PHP, Perl, Python.
Существует решение на базе ОС Windows Server, где в качестве веб-сервера используется IIS, СУБД MySQL, MS SQL а также поддержка платформы ASP.NET.
Разделение ресурсов на shared hosing основывается на ограничении дискового пространства, сетевого трафика, количества используемых доменов, почтовых ящиков, баз данных, FTP-аккаунтов, ограничение на использование процессорного времени, памяти для PHP-скриптов и так далее.

Виртуальный выделенный сервер эмулирует работу отдельного физического сервера.
На одной физической машине может быть запущено несколько виртуальных серверов, при этом каждый виртуальный сервер имеет свои процессы, ресурсы и отдельное администрирование.
Для реализации виртуальных машин используются технологии виртуализации, как системы с открытым исходным кодом, так и коммерческие.

В случае выделенного сервера, клиенту целиком предоставляется отдельная физическая машина.
Владелец сервера имеет возможность смены конфигурации оборудования, установки любой операционной системы.
Такой тип хостинга подходит для высоконагруженных проектов.

Colocation отличается от услуги предоставления выделенного сервера тем, что ДЦ размещает у себя сервер, который заранее подготовил клиент.
Дата-центр подключает его в общую инфраструктуру ЦОДа, обеспечивает бесперебойное электропитание, охлаждение, доступ к сетевому каналу, удаленный доступ к серверу, охрану, мониторинг и другие услуги.

Выделенная зона предоставляется в основном для специальных клиентов, имеющих строгие нормы безопасности.
В этом случае дата-центр предоставляет выделенную зону, обеспеченную электроснабжением, холодоснабжением и системами безопасности, а клиент сам создает свой дата-центр внутри этого пространства.

Также можно выделить такую услугу, как аренда телекоммуникационных стоек, которая является частным случаем colocation, с отличием в том, что арендаторами в основном являются юридические лица.

При построении облачной инфраструктуры важную роль играет виртуализация.

Виртуализация --- абстракция вычислительных ресурсов и предоставление пользователю системы, которая инкапсулирует (скрывает в себе) собственную реализацию.
Термин <<виртуализация>> появился в шестидесятых годах XX века, а в девяностых --- стали очевидны перспективы подхода: с ростом аппаратных мощностей, как персональных компьютеров, так и серверных решений, вскоре представится возможность использовать несколько виртуальных машин на одной физической платформе.

Понятие виртуализации можно условно разделить на две категории:
\begin{itemize}
  \item Виртуализация платформ, продуктом этого вида виртуализации являются виртуальные машины --- некие программные абстракции, запускаемые на платформе реально аппаратно-программных систем;
  \item Виртуализация ресурсов преследует целью комбинирование или упрощение представления аппаратных ресурсов для пользователя и получение неких пользовательских абстракций оборудования, пространств имен, сетей.
\end{itemize}

Когда производится виртуализация, существует не один способ ее осуществления.
Фактически есть несколько путей, с помощью которых достигаются одинаковые результаты через разные уровни абстракции: \cite{openvz-tutorial}
\begin{itemize}
  \item Эмуляция оборудования;
  \item Полная виртуализация;
  \item Паравиртуализация;
  \item Виртуализация уровня ОС.
\end{itemize}

Эмуляция оборудования является одним из самых сложных методов виртуализации.
В то же время, главной проблемой при эмуляции аппаратных средств является низкая скорость работы, в связи с тем, что каждая команда моделируется на основных аппаратных средствах. 
Однако метод позволяет использовать виртуализированные аппаратные средства еще до выхода реальных.
В эмуляции оборудования используется механизм динамической трансляции, то есть каждая из инструкций эмулируемой платформы заменяется на заранее подготовленный фрагмент инструкций физического процессора.
\addimghere{emulation}{0.35}{Эмуляция оборудования моделирует аппаратные средства}

Примерами виртуализации посредством эмуляции являются QEMU и Bochs.
QEMU поддерживает два режима эмуляции: пользовательский и системный.
Пользовательский режим эмуляции позволяет процессу, созданному на одном процессоре, работать на другом (выполняется динамический перевод инструкций для принимающего процессора и конвертация системных вызовов Linux).
Системный режим позволяет эмулировать систему целиком, включая процессор и разнообразную периферию.
Достоинством QEMU является его быстрый и компактный динамический транслятор.
Динамический транслятор позволяет во время исполнения переводить инструкции целевого (гостевого) процессора в инструкции центрального процессора хоста для обеспечения эмуляции.
QEMU обеспечивает динамическую трансляцию преобразованием целевой инструкции в микрооперации.
Эти микрооперации представляют собой элеемнты C-кода, которые компилируются в объекты.
Затем запускается основной транслятор, который отображает целевые инструкции на микрооперации для динамической трансляции.
Такой подход не только эффективен, но и обеспечивает переносимость.

Использование QEMU в качестве эмулятора персонального компьютера обеспечивает поддержку разнообразных периферийных устройств.
Естественно, сюда входят стандартные периферийные устройства --- эмулятор аппаратного видеоадаптера (VGA), мыши и клавиатуры PS/2, интерфейс IDE для жестких дисков, интерфейс CD-ROM и эмуляция дисковода.
Кроме того, QEMU имеет возможность эмуляции сетевых адаптеров NE2000 (PCI), последовательных портов, многочисленных звуковых плат и контроллера PCI Universal Host Controller Interface (UHCI), Universal Serial Bus (USB) (с виртуальным USB концентратором).
Также поддерживается до 255 процессоров с поддержкой симметричной многопроцессорности (SMP).

Полная виртуализация использует гипервизор, который осуществляет связь между гостевой ОС и аппаратными средствами физического сервера.
В связи с тем, что вся работа с гостевой операционной системой проходит через гипервизор, то скорость работы ниже чем в случае прямого взаимодействия с аппаратурой.
Основным преимуществом является то, что в ОС не вносятся никакие изменения, единственное ограничение --- операционная система должна поддерживать основные аппаратные средства.
\addimghere{full_virt}{0.35}{Полная виртуализация использует гипервизор}

KVM (Kernel-based Virtual Machine) --- программное решение, обеспечивающее виртуализацию в среде Linux, которая поддерживает аппаратную виртуализацию на базе Intel VT (Virtualization Technology) либо AMD SVM (Secure Virtual Machine).
KVM не выполняет никакой самоэмуляции, вместо этого, программа, работающая в пользовательском пространстве, применяет интерфейс /dev/kvm для настройки адресного пространства гостевого виртуального сервера, берет его смоделированные ресурсы ввода/вывода и отображает его образ на образ хоста.

В архитектуре KVM, виртуальная машина выполняется как обычный Linux-процесс, запланированный стандартным планировщиком Linux.
На самом деле виртуальный процессор представляется как обычный Linux-процесс, это позволяет KVM пользоваться всеми возможностями ядра Linux.
Эмуляцией устройств управляет модифицированная версия QEMU, которая обеспечивает эмуляцию BIOS, шины PCI, шины USB, а также стандартный набор устройств, таких как дисковые контроллеры IDE и SCSI, сетевые карты и другие.

Паравиртуализация имеет некоторые сходства с полной виртуализацией.
Этот метод использует гипервизор для разделения доступа к основным аппаратным средствам, но объединяет код, касающийся виртуализации, в непосредственно операционную систему, поэтому недостатком метода является то, что гостевая ОС должна быть изменена для гипервизора.
Но паравиртуализация существенно быстрее полной виртуализации, скорость работы виртуальной машины приближена к скорости реальной, это осуществляется за счет отсутствия эмуляции аппаратуры и учета существования гипервизора при выполнении системных вызово в коде ядра.
Вместо привилегированных операций совершаются гипервызовы обращения ядра гостевой ОС к гипервизору с просьбой о выполнении операции.
\addimghere{paravirt}{0.35}{Паравиртуализация разделяет процесс с гостевой ОС}

Xen --- это монитор виртуальных машин (VMM, Virtual Machine Monitor) или гипервизор с поддержкой паравиртуализации для процессоров x86 архитектуры, распространяющийся с открытым исходным кодом.
Xen может организовать совместное безопасное исполнение нескольких виртуальных машин на одной физической системе с производительностью близкой к непосредственной.
Он перекладывает большинство задач по поддержке аппаратуры на гостевую операционную систему, работающую в управляющей виртуальной машине, также известной как домен 0 (dom0).
Сам Xen содержит только код, необходимый для обнаружения и запуска остальных процессоров системы, настройки обработки прерываний и нумерации PCI шины.
Драйверы устройств работают внутри привилегированной гостевой операционной системы, а не в самом Xen.
Такой подход обеспечивает совместимость с большинством устройств, поддерживаемых Linux.
Сборка XenLinux по умолчанию содержит поддержку большинства серверного сетевого и дискового оборудования, но при необходимости можно добавить поддержку других устройств, переконфигурировав Linux-ядро стандартным способом.

В паравиртуальном режиме (PV) оборудование не эмулируется, и гостевая ОС должна быть специальным образом модифицирована, чтобы работать в таком окружении.
Начиная с версии 3.0, ядро Linux поддерживает запуск в паравиртуальном режиме без перекомпиляции со сторонними патчами.
Преимущество режима паравиртуализации в том, что он не требует поддержки аппаратной виртуализации со стороны CPU, а также не тратит вычислительные ресурсы для эмуляции оборудования на шине PCI.
Режим аппаратной виртуализации (HVM) появился в Xen, начиная с версии 3.0 гипервизора, и требует поддержки со стороны оборудования.
В этом режиме для эмуляции виртуальных устройств используется QEMU, который весьма неповоротлив даже с паравиртуальными драйверами.
Однако со временем поддержка аппаратной виртуализации в оборудовании получила настолько широкое распространение, что используется даже в CPU ноутбуков.
Поэтому у разработчиков возникло желание использовать быстрое переключение контекста исполнения между гипервизором и гостевой ОС и в паравиртуальном режиме, используя возможности обордования.
Так появился новый режим --- аппаратная паравиртуализация (PVH), который доступен в Xen с версии 4.4.

Виртуализация уровня операционной системы отличается от других.
Она использует технику, при которой сервера виртуализируются непосредственно над ОС.
Недостатком метода является то, что поддерживается одна единственная операционная система на физическом сервере, которая изолирует контейнеры друг от друга.
Преимуществом виртуализации уровня ОС является <<родная>> производительность.
Виртуализация уровня ОС --- метод виртуализации, при котором ядро операционной системы поддерживает несколько изолированных экземпляров пространства пользователя, вместо одного.
Эти экземпляры с точки зрения пользователя полностью идентичны реальному серверу.
Для систем на базе UNIX, эта технология может рассматриваться как улучшенная реализация механизма chroot.
Ядро обеспечивает полную изолированность контейнеров, поэтому программы из разных контейнеров не могут воздействовать друг на друга.
\addimghere{cont_virt}{0.35}{Виртуализация уровня ОС изолирует серверы}

OpenVZ разрабатывается как патч к исходным текстам ядра Linux.
В модифицированном ядре добавлен массив дополнительных сущностей --- виртуальных окружений (VE), а для всех имеющихся объектов (процессы, сокеты и прочие) введены дополнительные поля --- номер VE, к которому этот объект относится, и номер объекта внутри VE.
Каждое виртуальное окружение имеет собственный набор квот на потребление системных ресурсов и отдельный каталог для использования в качестве корневой файловой системы.
Дополнительные модули ядра --- vzdev, vzmon и прочие, отвечают за работу ограничений, мониторинг, эмуляцию сети в VE, сохранение и восстановление текущего состояния запущенных контейнеров.
К преимуществам OpenVZ по сравнению с более универсальными инструментами виртуализации, такими как Xen и KVM, является прозрачный доступ из внешней системы к процессам, файлам и прочим ресурсам в гостевых.

OpenVZ разрабатывается фирмой Parallels как часть более крупного коммерческого продукта под названием Parallels Virtuozzo Containers (PVC).
В число преимуществ Virtuozzo, по сравнению с OpenVZ, входят:
\begin{itemize}
  \item Файловая система VZFS;
  \item Управление через графическую консоль и веб-интерфейс;
  \item Программный интерфейс на базе XML для создания собственных инструментов управления и контроля;
  \item Средства миграции с физическом системы в контейнер и обратно;
  \item Средства контроля за полосой и суммарным потреблением трафика;
  \item Интеграция с Plesk, коммерческой панелью управления хостингом той же фирмы;
  \item Круглосуточная техническая поддержка.
\end{itemize}

VZFS позволяет совмещать файловые системы контейнеров, при этом базовый образ используется всеми контейнерами, а изменения в нем для каждого контейнера сохраняются раздельно.
Преимущества такого подхода:
\begin{itemize}
  \item Место, занимаемое программами на диске, становится фиксированным и не зависит от количества контейнеров, в которые эти программы инсталлированы;
  \item Уменьшается расход ОЗУ, так как код нескольких экземпляров программы или библиотеки, запущенной из одного и того же исполняемого файла, размещается в ОЗУ в единственном экземпляре;
  \item Обновление программного обеспечения в группе контейнеров выполняется одной командой.
\end{itemize}

LXC (Linux Containers) --- система виртуализации на уровне операционной системы.
Данная система сходна с OpenVZ и Linux-VServer для Linux, а также FreeBSD jail и Solaris Containers.
LXC основана на технологии cgroups, входящей в ядро Linux, начиная с версии 2.6.29.
Ее нельзя рассматривать как законченный продукт.
Фактически это набор из нескольких совершенно самостоятельных функций ядра Linux и пользовательских утилит, которые позволяют удобно создавать и управлять изолированными контейнерами.
Практически вся функциональность LXC представления известными механизмами ядра:
\begin{itemize}
  \item cgroups;
  \item Пространства имен (namespaces).
\end{itemize}

cgroups (Control Groups) --- позволяет ограничить аппаратные ресурсы некоторого набора процессов.
Под аппаратными ресурсами подразумеваются: процессорное время, память, дисковая и сетевая подсистемы.
Набор или группа процессов может быть определен различным критериям.
Например, это может быть целая иерархия процессов, получающая все лимиты родительского процесса.
Кроме этого, возможен подсчет расходуемых группой ресурсов, заморозка (freezing) групп, создание контрольных точке (checkpointing) и их перезагрузка.
Для управления этим полезным механизмом существует специальная библиотека libcgroups, в состав которой входят такие утилиты, как cgcreate, cgexec и некоторые другие.

namespaces --- пространства имен.
Это механизм ядра, который позволяет изолировать процессы друг от друга.
Изоляция может быть выполнена в шести контекстах (пространствах имен):
\begin{itemize}
  \item Mount --- предоставляет процессам собственную иерархию файловой системы и изолирует ее от других таких же иерархий, по аналогии с chroot;
  \item PID --- изолирует идентификаторы процессов (PID) одного пространства имен от процессов с такими же идентификаторами другого пространства;
  \item Network --- предоставляет отдельным процессам логически изолированный от других стек сетевых протоколов, сетевой интерфейс, IP-адрес, таблицу маршрутизации, ARP и прочие реквизиты;
  \item IPC --- обеспечивает разделяемую память и взаимодействие между процессами;
  \item UTS --- изоляция идентификаторов узла, такого как имя хоста (hostname) и домена (domainname);
  \item User --- позволяет иметь один и тот же набор пользователей и групп в рамках разных пространств имен, в каждом контейнере могут быть свой root и любые другие пользователи и группы.
\end{itemize}

Одно из главных преимуществ LXC --- это присутствие его базовых блоков (cgroups и namespaces) во всех современных ядрах Linux.
Это означает, что нет необходимости что-то компилировать или использовать стороннее ядро, как в случае с OpenVZ.
Единственное, что необходимо установить, это пакет утилит управления.

К системам управления можно отнести Docker.

Docker --- программное обеспечение для автоматизации развертывания и управления приложениями в среде виртуализации на уровне операционной системы, например LXC.
Позволяет <<упаковать>> приложение со всем его окружением и зависимостями в контейнер, который может быть перенесен на любой Linux-системе с поддержкой cgroups в ядре, а также предоставляет среду по управлению контейнерами.

Для экономии дискового пространства проект использует файловую систему Aufs с поддержкой каскадно-объединенного монтирования: контейнеры используют образ базовой операционной системы, а изменения записываются в отдельную область.
Также поддерживается размещение контейнеров в файловой системе Btrfs с включенным режимом копирования при записи.
В состав программных средств входит демон --- сервер контейнеров, клиентские средства, позволяющие из интерфейса командной строки управлять образами и контейнерами, а также API, позволяющий в стиле REST управлять контейнерами программно.
Демон обеспечивает полную изоляцию запускаемых на узле контейнеров на уровне файловой системы (у каждого контейнера собственная файловая система), на уровне процессов (процессы имеют доступ только к собственной файловой системе контейнера, а ресурсы разделены средствами LXC), на уровне сети (каждый контейнер имеет доступ только к привязанному к нему сетевому пространству имен и соответствующим виртуальным сетевым интерфейсам).

Набор клиентских средств позволяет запускать процессы в новых контейнерах, останавливать и запускать контейнеры, приостанавливать и возобновлять процессы в контейнерах.
Серия команд позволяет осуществлять мониторинг запущенных процессов (по аналогии с ps, top).
Новые образы возможно создавать из специального сценарного файла (dockerfile), возможно записать все изменения, сделанные в контейнере в новый образ.
Все команды могут работать как с docker-демоном локальной системы, так и с любым сервером docker, доступным по сети.
Кроме того, в интерфейсе командной строки встроены возможности по взаимодействию с публичным репозиторием Docker Hub, в котором размещены предварительно собранные образы контейнеров, образы можно скачивать в локальную систему, возможно также отправить локально собранные образы в Docker Hub.

\clearpage
